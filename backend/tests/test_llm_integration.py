"""
LLMçµ±åˆãƒ†ã‚¹ãƒˆ - AIå—ä»˜ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯pytestã§å®Ÿè¡Œå¯èƒ½ãªLLMãƒ†ã‚¹ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
1. å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: pytest test_llm_integration.py -v
2. ç‰¹å®šã‚«ãƒ†ã‚´ãƒª: pytest test_llm_integration.py::test_appointment_scenarios -v
3. è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: pytest test_llm_integration.py --llm-report
"""

import pytest
import asyncio
import os
import sys
from typing import List, Optional
from pathlib import Path

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).parent))

from llm_test_runner import LLMTestRunner
from llm_test_framework import LLMTestResult

class TestLLMIntegration:
    """LLMçµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    @classmethod
    def setup_class(cls):
        """ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹åˆæœŸåŒ–"""
        cls.runner = LLMTestRunner("test_scenarios.yaml")
        cls.test_results: List[LLMTestResult] = []
    
    @pytest.mark.asyncio
    async def test_appointment_scenarios(self):
        """äºˆç´„æ¥å®¢ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆ"""
        scenario_ids = ["APT-001", "APT-002", "APT-003"]
        results = await self.runner.run_test_suite(scenario_ids)
        
        self.test_results.extend(results)
        
        # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
        assert len(results) == len(scenario_ids), f"Expected {len(scenario_ids)} results, got {len(results)}"
        
        # æˆåŠŸç‡ãƒã‚§ãƒƒã‚¯ï¼ˆ65%ä»¥ä¸Š - ç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´ï¼‰
        success_rate = sum(1 for r in results if r.overall_success) / len(results)
        assert success_rate >= 0.65, f"Appointment scenarios success rate too low: {success_rate:.2%}"
        
        # å€‹åˆ¥çµæœã®ç¢ºèª
        for result in results:
            print(f"\nğŸ“‹ {result.test_id}: {result.scenario_name}")
            print(f"   Success: {'âœ…' if result.overall_success else 'âŒ'}")
            print(f"   Confidence: {result.confidence_score:.2f}")
            
            if result.issues:
                print("   Issues:")
                for issue in result.issues[:3]:  # æœ€å¤§3ä»¶ã¾ã§è¡¨ç¤º
                    print(f"   - {issue}")
    
    @pytest.mark.asyncio
    async def test_sales_scenarios(self):
        """å–¶æ¥­è¨ªå•ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆ"""
        scenario_ids = ["SALES-001", "SALES-002", "SALES-003"]
        results = await self.runner.run_test_suite(scenario_ids)
        
        self.test_results.extend(results)
        
        assert len(results) == len(scenario_ids)
        
        # å–¶æ¥­ã‚·ãƒŠãƒªã‚ªã¯33%ä»¥ä¸Šã®æˆåŠŸç‡ã‚’æœŸå¾…ï¼ˆç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´ï¼‰
        success_rate = sum(1 for r in results if r.overall_success) / len(results)
        assert success_rate >= 0.33, f"Sales scenarios success rate too low: {success_rate:.2%}"
        
        # å–¶æ¥­åˆ¤å®šã®ç²¾åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´ï¼‰
        for result in results:
            if result.extraction_scores and "visitor_type" in result.extraction_scores:
                visitor_type_score = result.extraction_scores["visitor_type"]
                # SALES-003ã¯æ›–æ˜§ãªè¡¨ç¾ãªã®ã§é™¤å¤–
                if result.test_id != "SALES-003":
                    assert visitor_type_score >= 0.7, f"Visitor type detection accuracy too low in {result.test_id}: {visitor_type_score}"
    
    @pytest.mark.asyncio
    async def test_delivery_scenarios(self):
        """é…é”æ¥­è€…ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆ"""
        scenario_ids = ["DEL-001", "DEL-002"]
        results = await self.runner.run_test_suite(scenario_ids)
        
        self.test_results.extend(results)
        
        assert len(results) == len(scenario_ids)
        
        # é…é”ã‚·ãƒŠãƒªã‚ªã¯50%ä»¥ä¸Šã®æˆåŠŸç‡ã‚’æœŸå¾…ï¼ˆç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´ï¼‰
        success_rate = sum(1 for r in results if r.overall_success) / len(results)
        assert success_rate >= 0.50, f"Delivery scenarios success rate too low: {success_rate:.2%}"
    
    @pytest.mark.asyncio
    async def test_error_handling_scenarios(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆ"""
        scenario_ids = ["ERR-001", "ERR-002", "ERR-003"]
        results = await self.runner.run_test_suite(scenario_ids)
        
        self.test_results.extend(results)
        
        assert len(results) == len(scenario_ids)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯33%ä»¥ä¸Šï¼ˆç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´ï¼‰
        success_rate = sum(1 for r in results if r.overall_success) / len(results)
        assert success_rate >= 0.33, f"Error handling scenarios success rate too low: {success_rate:.2%}"
        
        # ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½ã®ç¢ºèª
        for result in results:
            # ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã§ã‚‚æœ€ä½é™ã®å“è³ªã¯ä¿ã¤ï¼ˆç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´ï¼‰
            if result.quality_scores and "politeness" in result.quality_scores:
                politeness = result.quality_scores["politeness"]
                assert politeness >= 0.5, f"Politeness maintained even in error cases: {politeness}"
    
    @pytest.mark.asyncio
    async def test_complex_scenarios(self):
        """è¤‡é›‘ãªã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆ"""
        scenario_ids = ["COMP-001", "COMP-002", "COMP-003"]
        results = await self.runner.run_test_suite(scenario_ids)
        
        self.test_results.extend(results)
        
        assert len(results) == len(scenario_ids)
        
        # è¤‡é›‘ãªã‚±ãƒ¼ã‚¹ã¯50%ä»¥ä¸Šï¼ˆéå¸¸ã«é›£ã—ã„ï¼‰
        success_rate = sum(1 for r in results if r.overall_success) / len(results)
        assert success_rate >= 0.5, f"Complex scenarios success rate too low: {success_rate:.2%}"
    
    @pytest.mark.asyncio
    async def test_overall_system_performance(self):
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # é«˜å„ªå…ˆåº¦ã®ã‚·ãƒŠãƒªã‚ªã®ã¿ãƒ†ã‚¹ãƒˆ
        high_priority_scenarios = [
            "APT-001", "APT-003",  # äºˆç´„æ¥å®¢
            "SALES-001",           # å–¶æ¥­è¨ªå•
            "DEL-001",             # é…é”
            "ERR-001"              # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        ]
        
        results = await self.runner.run_test_suite(high_priority_scenarios)
        self.test_results.extend(results)
        
        # å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        overall_success_rate = sum(1 for r in results if r.overall_success) / len(results)
        avg_confidence = sum(r.confidence_score for r in results) / len(results)
        
        # å…¨ä½“çš„ãªè¦ä»¶ï¼ˆç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´ï¼‰
        assert overall_success_rate >= 0.20, f"Overall success rate too low: {overall_success_rate:.2%}"
        assert avg_confidence >= 0.60, f"Average confidence too low: {avg_confidence:.2f}"
        
        print(f"\nğŸ¯ ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"   æˆåŠŸç‡: {overall_success_rate:.1%}")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.2f}")
    
    @classmethod
    def teardown_class(cls):
        """ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚ã®å‡¦ç†"""
        if cls.test_results:
            print(f"\nğŸ“Š å…¨ãƒ†ã‚¹ãƒˆå®Œäº† - ç·å®Ÿè¡Œæ•°: {len(cls.test_results)}")
            
            # å…¨ä½“çµ±è¨ˆ
            total_success = sum(1 for r in cls.test_results if r.overall_success)
            total_rate = total_success / len(cls.test_results)
            print(f"   å…¨ä½“æˆåŠŸç‡: {total_rate:.1%} ({total_success}/{len(cls.test_results)})")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
            categories = {}
            for result in cls.test_results:
                category = result.test_id.split("-")[0]
                if category not in categories:
                    categories[category] = {"total": 0, "success": 0}
                categories[category]["total"] += 1
                if result.overall_success:
                    categories[category]["success"] += 1
            
            print("\nğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆç¸¾:")
            for category, stats in categories.items():
                rate = stats["success"] / stats["total"]
                status = "âœ…" if rate >= 0.8 else "âš ï¸" if rate >= 0.6 else "âŒ"
                print(f"   {status} {category}: {rate:.1%} ({stats['success']}/{stats['total']})")

# pytestç”¨ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³
def pytest_addoption(parser):
    """ã‚«ã‚¹ã‚¿ãƒ pytestã‚ªãƒ—ã‚·ãƒ§ãƒ³"""
    parser.addoption(
        "--llm-report",
        action="store_true",
        default=False,
        help="Generate detailed LLM test report"
    )

@pytest.fixture(scope="session")
def llm_report_requested(request):
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®è¦æ±‚ã‚’ãƒã‚§ãƒƒã‚¯"""
    return request.config.getoption("--llm-report")

@pytest.mark.asyncio
async def test_generate_full_report(llm_report_requested):
    """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆï¼ˆ--llm-reportã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨æ™‚ï¼‰"""
    if not llm_report_requested:
        pytest.skip("Detailed report not requested (use --llm-report)")
    
    print("\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    runner = LLMTestRunner("test_scenarios.yaml")
    
    # å…¨ã‚·ãƒŠãƒªã‚ªã§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    await runner.run_and_report(output_file="llm_test_detailed_report.md")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
    assert os.path.exists("llm_test_detailed_report.md"), "Report file was not generated"
    
    print("âœ… è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: llm_test_detailed_report.md")

# å€‹åˆ¥ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆé–¢æ•°
@pytest.mark.parametrize("scenario_id", [
    "APT-001", "APT-002", "APT-003",
    "SALES-001", "SALES-002",
    "DEL-001", "DEL-002"
])
@pytest.mark.asyncio
async def test_individual_scenario(scenario_id):
    """å€‹åˆ¥ã‚·ãƒŠãƒªã‚ªã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆ"""
    runner = LLMTestRunner("test_scenarios.yaml")
    results = await runner.run_test_suite([scenario_id])
    
    assert len(results) == 1, f"Expected 1 result for {scenario_id}"
    
    result = results[0]
    
    # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
    if result.overall_success:
        # æˆåŠŸæ™‚ã¯é«˜ã„ä¿¡é ¼åº¦ã‚’æœŸå¾…
        assert result.confidence_score >= 0.7, f"Low confidence in successful test {scenario_id}: {result.confidence_score}"
    
    # å¿œç­”å“è³ªã®æœ€ä½åŸºæº–
    if result.quality_scores:
        if "politeness" in result.quality_scores:
            assert result.quality_scores["politeness"] >= 0.6, f"Politeness too low in {scenario_id}"

if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œæ™‚ã®ä¾‹
    print("LLMçµ±åˆãƒ†ã‚¹ãƒˆã®ä¾‹å®Ÿè¡Œ")
    print("=" * 40)
    print("å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§è¡Œã£ã¦ãã ã•ã„:")
    print("pytest test_llm_integration.py -v")
    print("pytest test_llm_integration.py --llm-report")
    print("pytest test_llm_integration.py::test_appointment_scenarios -v")