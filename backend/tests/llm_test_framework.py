"""
LLMãƒ™ãƒ¼ã‚¹AIå—ä»˜ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

ç›®çš„ï¼š
- æƒ³å®šã—ãŸæŒ™å‹•ã«ãªã£ã¦ã„ã‚‹ã‹ã‚’å®šé‡çš„ãƒ»å®šæ€§çš„ã«åˆ¤å®š
- å•é¡Œç‚¹ã¨å…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ˜ç¢ºåŒ–
- ä¿®æ­£å‰å¾Œã®æ¯”è¼ƒã«ã‚ˆã‚‹æ”¹å–„ç¢ºèª
"""

import pytest
import asyncio
import yaml
import re
import json
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime
import statistics
from pathlib import Path

@dataclass
class ExtractionResult:
    """æƒ…å ±æŠ½å‡ºçµæœ"""
    name: Optional[str] = None
    company: Optional[str] = None
    visitor_type: Optional[str] = None
    purpose: Optional[str] = None
    confidence: float = 0.0

@dataclass
class TestEvidence:
    """ãƒ†ã‚¹ãƒˆè¨¼è·¡"""
    test_id: str
    input_message: str
    expected: Dict[str, Any]
    actual: Dict[str, Any]
    passed: bool
    issues: List[str]
    timestamp: str

@dataclass
class ImprovementSuggestion:
    """æ”¹å–„ææ¡ˆ"""
    category: str  # "prompt", "logic", "config"
    problem: str
    evidence: List[str]
    suggested_fix: str
    file_to_modify: str
    priority: str  # "high", "medium", "low"

@dataclass
class TestResult:
    """ãƒ†ã‚¹ãƒˆçµæœï¼ˆè©³ç´°ãªåˆ¤æ–­åŸºæº–ä»˜ãï¼‰"""
    test_id: str
    scenario_name: str
    overall_success: bool
    confidence_score: float
    
    # è©³ç´°è©•ä¾¡
    extraction_scores: Dict[str, float]  # name: 1.0, company: 1.0, visitor_type: 0.0
    flow_scores: Dict[str, float]        # state_transition: 1.0, error_handling: 0.5
    quality_scores: Dict[str, float]     # politeness: 0.9, clarity: 0.8
    
    # åˆ¤æ–­è©³ç´°
    judgements: Dict[str, str]           # "name_extraction": "âœ… æ­£ç¢º"
    issues: List[str]                    # å…·ä½“çš„ãªå•é¡Œç‚¹
    evidence: TestEvidence
    
    # æ”¹å–„ææ¡ˆ
    suggestions: List[ImprovementSuggestion]

class DetailedValidator:
    """è©³ç´°ãªåˆ¤æ–­åŸºæº–ã‚’æä¾›ã™ã‚‹ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.extraction_patterns = self._load_extraction_patterns()
        self.quality_criteria = self._load_quality_criteria()
    
    def _load_extraction_patterns(self) -> Dict[str, Any]:
        """æƒ…å ±æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©"""
        return {
            "name_patterns": [
                r"([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]{2,10})(?:ã§ã™|ã¨ç”³ã—ã¾ã™|ã¨ã„ã„ã¾ã™)",
                r"ç§ã¯([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]{2,10})",
                r"([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼ä¸€-é¾ ]{2,10})(?:ã•ã‚“|æ§˜|æ°)",
            ],
            "company_patterns": [
                r"([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼a-zA-Z0-9]{2,20})(?:æ ªå¼ä¼šç¤¾|ä¼šç¤¾|å•†äº‹|ã‚°ãƒ«ãƒ¼ãƒ—|Corp|Inc)",
                r"([ã-ã‚“ã‚¡-ãƒ¶ãƒ¼a-zA-Z0-9]{2,20})(?:ã‹ã‚‰|ã‚ˆã‚Š)(?:æ¥|å‚)",
            ],
            "visitor_type_keywords": {
                "appointment": ["ä¼šè­°", "ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "æ‰“ã¡åˆã‚ã›", "é¢è«‡", "ã‚¢ãƒã‚¤ãƒ³ãƒˆ", "äºˆç´„", "æ™‚"],
                "sales": ["å–¶æ¥­", "æ¡ˆå†…", "ç´¹ä»‹", "ææ¡ˆ", "å•†å“", "ã‚µãƒ¼ãƒ“ã‚¹", "æ–°è¦"],
                "delivery": ["é…é”", "ãŠå±Šã‘", "è·ç‰©", "å®…é…", "é‹è¼¸", "ä¾¿"]
            }
        }
    
    def _load_quality_criteria(self) -> Dict[str, Any]:
        """å“è³ªåŸºæº–ã®å®šç¾©"""
        return {
            "politeness_indicators": ["ã§ã™", "ã¾ã™", "ã”ã–ã„ã¾ã™", "ã„ãŸã—ã¾ã™", "ã•ã›ã¦ã„ãŸã ã"],
            "clarity_indicators": ["ç¢ºèª", "ãŠèã‹ã›", "æ•™ãˆã¦", "ã«ã¤ã„ã¦"],
            "forbidden_phrases": ["ã‚ã‹ã‚‰ãªã„", "ã‚€ã‚Š", "ã ã‚"],  # "ã§ãã¾ã›ã‚“"ã‚’å‰Šé™¤ï¼ˆæ­£å½“ãªä½¿ç”¨ã‚‚ã‚ã‚‹ï¼‰
            "max_response_length": 250,  # ã‚ˆã‚Šç¾å®Ÿçš„ãªé•·ã•ã«å¤‰æ›´
            "min_response_length": 20
        }
    
    def validate_extraction(self, actual: ExtractionResult, expected: Dict[str, Any]) -> Tuple[Dict[str, float], Dict[str, str], List[str]]:
        """æƒ…å ±æŠ½å‡ºã®è©³ç´°è©•ä¾¡"""
        scores = {}
        judgements = {}
        issues = []
        
        # åå‰ã®è©•ä¾¡
        if "name" in expected:
            expected_name = expected["name"]
            if actual.name:
                if self._names_match(actual.name, expected_name):
                    scores["name"] = 1.0
                    judgements["name_extraction"] = f"âœ… æ­£ç¢º ({actual.name})"
                else:
                    scores["name"] = 0.5
                    judgements["name_extraction"] = f"âš ï¸ éƒ¨åˆ†çš„ (æœŸå¾…:{expected_name}, å®Ÿéš›:{actual.name})"
                    issues.append(f"åå‰æŠ½å‡º: æœŸå¾…ã€Œ{expected_name}ã€vså®Ÿéš›ã€Œ{actual.name}ã€")
            else:
                scores["name"] = 0.0
                judgements["name_extraction"] = "âŒ æŠ½å‡ºå¤±æ•—"
                issues.append("åå‰ãŒæŠ½å‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ä¼šç¤¾åã®è©•ä¾¡
        if "company" in expected:
            expected_company = expected["company"]
            if actual.company:
                if self._companies_match(actual.company, expected_company):
                    scores["company"] = 1.0
                    judgements["company_extraction"] = f"âœ… æ­£ç¢º ({actual.company})"
                else:
                    scores["company"] = 0.5
                    judgements["company_extraction"] = f"âš ï¸ éƒ¨åˆ†çš„ (æœŸå¾…:{expected_company}, å®Ÿéš›:{actual.company})"
                    issues.append(f"ä¼šç¤¾åæŠ½å‡º: æœŸå¾…ã€Œ{expected_company}ã€vså®Ÿéš›ã€Œ{actual.company}ã€")
            else:
                scores["company"] = 0.0
                judgements["company_extraction"] = "âŒ æŠ½å‡ºå¤±æ•—"
                issues.append("ä¼šç¤¾åãŒæŠ½å‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # è¨ªå•è€…ã‚¿ã‚¤ãƒ—ã®è©•ä¾¡
        if "visitor_type" in expected:
            expected_type = expected["visitor_type"]
            if actual.visitor_type == expected_type:
                scores["visitor_type"] = 1.0
                judgements["visitor_type_classification"] = f"âœ… æ­£ç¢º ({actual.visitor_type})"
            else:
                scores["visitor_type"] = 0.0
                judgements["visitor_type_classification"] = f"âŒ èª¤åˆ†é¡ (æœŸå¾…:{expected_type}, å®Ÿéš›:{actual.visitor_type})"
                issues.append(f"è¨ªå•è€…ã‚¿ã‚¤ãƒ—åˆ†é¡: æœŸå¾…ã€Œ{expected_type}ã€vså®Ÿéš›ã€Œ{actual.visitor_type}ã€")
        
        return scores, judgements, issues
    
    def validate_response_quality(self, response_text: str) -> Tuple[Dict[str, float], Dict[str, str], List[str]]:
        """å¿œç­”å“è³ªã®è©³ç´°è©•ä¾¡"""
        scores = {}
        judgements = {}
        issues = []
        
        # ä¸å¯§ã•ã®è©•ä¾¡
        politeness_count = sum(1 for phrase in self.quality_criteria["politeness_indicators"] 
                             if phrase in response_text)
        politeness_score = min(1.0, politeness_count / 2)  # 2ã¤ä»¥ä¸Šã§æº€ç‚¹
        scores["politeness"] = politeness_score
        
        if politeness_score >= 0.8:
            judgements["politeness"] = f"âœ… é©åˆ‡ (æ•¬èªè¡¨ç¾{politeness_count}å€‹)"
        elif politeness_score >= 0.5:
            judgements["politeness"] = f"âš ï¸ æ”¹å–„ã®ä½™åœ° (æ•¬èªè¡¨ç¾{politeness_count}å€‹)"
        else:
            judgements["politeness"] = f"âŒ ä¸é©åˆ‡ (æ•¬èªè¡¨ç¾{politeness_count}å€‹)"
            issues.append("æ•¬èªè¡¨ç¾ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        
        # æ˜ç¢ºã•ã®è©•ä¾¡
        length = len(response_text)
        if self.quality_criteria["min_response_length"] <= length <= self.quality_criteria["max_response_length"]:
            scores["clarity"] = 1.0
            judgements["clarity"] = f"âœ… é©åˆ‡ãªé•·ã• ({length}æ–‡å­—)"
        elif length > self.quality_criteria["max_response_length"]:
            scores["clarity"] = 0.6
            judgements["clarity"] = f"âš ï¸ é•·ã™ãã‚‹ ({length}æ–‡å­—)"
            issues.append(f"å¿œç­”ãŒé•·ã™ãã¾ã™ ({length}æ–‡å­— > {self.quality_criteria['max_response_length']}æ–‡å­—)")
        else:
            scores["clarity"] = 0.4
            judgements["clarity"] = f"âš ï¸ çŸ­ã™ãã‚‹ ({length}æ–‡å­—)"
            issues.append(f"å¿œç­”ãŒçŸ­ã™ãã¾ã™ ({length}æ–‡å­— < {self.quality_criteria['min_response_length']}æ–‡å­—)")
        
        # ç¦æ­¢ãƒ•ãƒ¬ãƒ¼ã‚ºãƒã‚§ãƒƒã‚¯
        forbidden_found = [phrase for phrase in self.quality_criteria["forbidden_phrases"] 
                          if phrase in response_text]
        if forbidden_found:
            scores["appropriateness"] = 0.0
            judgements["appropriateness"] = f"âŒ ä¸é©åˆ‡ãªè¡¨ç¾ ({', '.join(forbidden_found)})"
            issues.append(f"ä¸é©åˆ‡ãªè¡¨ç¾ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {', '.join(forbidden_found)}")
        else:
            scores["appropriateness"] = 1.0
            judgements["appropriateness"] = "âœ… é©åˆ‡ãªè¡¨ç¾"
        
        return scores, judgements, issues
    
    def _names_match(self, actual: str, expected: str) -> bool:
        """åå‰ã®ä¸€è‡´åˆ¤å®š"""
        # æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
        actual_norm = re.sub(r'[ã€€\s]', '', actual)
        expected_norm = re.sub(r'[ã€€\s]', '', expected)
        return actual_norm == expected_norm
    
    def _companies_match(self, actual: str, expected: str) -> bool:
        """ä¼šç¤¾åã®ä¸€è‡´åˆ¤å®š"""
        # ä¼šç¤¾è¡¨è¨˜ã®æ­£è¦åŒ–
        actual_norm = self._normalize_company(actual)
        expected_norm = self._normalize_company(expected)
        return actual_norm == expected_norm
    
    def _normalize_company(self, company: str) -> str:
        """ä¼šç¤¾åã®æ­£è¦åŒ–"""
        normalized = re.sub(r'æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾|\(æ ª\)|\(æœ‰\)|Co\.|Corp\.|Inc\.', '', company)
        normalized = re.sub(r'[ã€€\s]', '', normalized)
        return normalized.strip()

class AnalysisEngine:
    """ãƒ†ã‚¹ãƒˆçµæœã®åˆ†æã¨æ”¹å–„ææ¡ˆã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def analyze_test_results(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœã®åŒ…æ‹¬çš„åˆ†æ"""
        analysis = {
            "overall_metrics": self._calculate_overall_metrics(test_results),
            "category_performance": self._analyze_by_category(test_results),
            "failure_patterns": self._identify_failure_patterns(test_results),
            "improvement_suggestions": self._generate_improvement_suggestions(test_results),
            "priority_actions": self._prioritize_actions(test_results)
        }
        return analysis
    
    def _calculate_overall_metrics(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        if not test_results:
            return {}
        
        success_rate = sum(1 for r in test_results if r.overall_success) / len(test_results)
        avg_confidence = statistics.mean(r.confidence_score for r in test_results)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚³ã‚¢
        extraction_scores = []
        quality_scores = []
        
        for result in test_results:
            if result.extraction_scores:
                extraction_scores.extend(result.extraction_scores.values())
            if result.quality_scores:
                quality_scores.extend(result.quality_scores.values())
        
        return {
            "total_tests": len(test_results),
            "success_rate": success_rate,
            "avg_confidence": avg_confidence,
            "avg_extraction_score": statistics.mean(extraction_scores) if extraction_scores else 0,
            "avg_quality_score": statistics.mean(quality_scores) if quality_scores else 0,
            "failed_tests": len(test_results) - sum(1 for r in test_results if r.overall_success)
        }
    
    def _analyze_by_category(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆ†æ"""
        categories = {}
        
        for result in test_results:
            # ãƒ†ã‚¹ãƒˆIDã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º (ä¾‹: APT-001 -> APT)
            category = result.test_id.split("-")[0] if "-" in result.test_id else "UNKNOWN"
            
            if category not in categories:
                categories[category] = {
                    "total": 0,
                    "success": 0,
                    "avg_confidence": 0.0,
                    "avg_extraction_score": 0.0,
                    "avg_quality_score": 0.0,
                    "test_ids": []
                }
            
            categories[category]["total"] += 1
            categories[category]["test_ids"].append(result.test_id)
            
            if result.overall_success:
                categories[category]["success"] += 1
            
            # ã‚¹ã‚³ã‚¢ã®ç´¯ç©
            categories[category]["avg_confidence"] += result.confidence_score
            
            if result.extraction_scores:
                avg_ext = statistics.mean(result.extraction_scores.values())
                categories[category]["avg_extraction_score"] += avg_ext
            
            if result.quality_scores:
                avg_qual = statistics.mean(result.quality_scores.values())
                categories[category]["avg_quality_score"] += avg_qual
        
        # å¹³å‡å€¤ã‚’è¨ˆç®—
        for category_data in categories.values():
            total = category_data["total"]
            if total > 0:
                category_data["success_rate"] = category_data["success"] / total
                category_data["avg_confidence"] /= total
                category_data["avg_extraction_score"] /= total
                category_data["avg_quality_score"] /= total
        
        return categories
    
    def _identify_failure_patterns(self, test_results: List[TestResult]) -> List[Dict[str, Any]]:
        """å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š"""
        patterns = {}
        
        for result in test_results:
            if not result.overall_success:
                for issue in result.issues:
                    if issue not in patterns:
                        patterns[issue] = {
                            "count": 0,
                            "test_ids": [],
                            "examples": []
                        }
                    patterns[issue]["count"] += 1
                    patterns[issue]["test_ids"].append(result.test_id)
                    patterns[issue]["examples"].append({
                        "input": result.evidence.input_message,
                        "expected": result.evidence.expected,
                        "actual": result.evidence.actual
                    })
        
        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_patterns = sorted(patterns.items(), key=lambda x: x[1]["count"], reverse=True)
        
        return [
            {
                "pattern": pattern,
                "frequency": data["count"],
                "affected_tests": data["test_ids"],
                "examples": data["examples"][:3]  # æœ€å¤§3ä¾‹
            }
            for pattern, data in sorted_patterns
        ]
    
    def _generate_improvement_suggestions(self, test_results: List[TestResult]) -> List[ImprovementSuggestion]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        # å…¨ã¦ã®ææ¡ˆã‚’åé›†
        for result in test_results:
            suggestions.extend(result.suggestions)
        
        # é‡è¤‡é™¤å»ã¨å„ªå…ˆåº¦ä»˜ã‘
        unique_suggestions = {}
        for suggestion in suggestions:
            key = f"{suggestion.category}_{suggestion.problem}"
            if key not in unique_suggestions:
                unique_suggestions[key] = suggestion
            else:
                # è¨¼æ‹ ã‚’çµ±åˆ
                unique_suggestions[key].evidence.extend(suggestion.evidence)
        
        return list(unique_suggestions.values())

class TestReportGenerator:
    """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    def generate_detailed_report(self, analysis: Dict[str, Any], test_results: List[TestResult]) -> str:
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = []
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        report.append("# AIå—ä»˜ã‚·ã‚¹ãƒ†ãƒ  LLMãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ")
        report.append(f"ç”Ÿæˆæ—¥æ™‚: {timestamp}\n")
        
        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
        metrics = analysis["overall_metrics"]
        report.append("## ğŸ“Š ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼")
        report.append(f"- **å…¨ä½“æˆåŠŸç‡**: {metrics['success_rate']:.1%} ({metrics['total_tests'] - metrics['failed_tests']}/{metrics['total_tests']})")
        report.append(f"- **å¹³å‡ä¿¡é ¼åº¦**: {metrics['avg_confidence']:.2f}")
        report.append(f"- **æƒ…å ±æŠ½å‡ºç²¾åº¦**: {metrics['avg_extraction_score']:.1%}")
        report.append(f"- **å¿œç­”å“è³ª**: {metrics['avg_quality_score']:.1%}")
        report.append("")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        if "category_performance" in analysis:
            report.append("## ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
            for category, perf in analysis["category_performance"].items():
                status = "âœ…" if perf["success_rate"] > 0.9 else "âš ï¸" if perf["success_rate"] > 0.7 else "âŒ"
                report.append(f"- {status} **{category}**: {perf['success_rate']:.1%}")
            report.append("")
        
        # ä¸»è¦å•é¡Œ
        report.append("## âš ï¸ ä¸»è¦å•é¡Œã¨æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        for i, pattern in enumerate(analysis["failure_patterns"][:5], 1):
            report.append(f"### {i}. {pattern['pattern']} (ç™ºç”Ÿå›æ•°: {pattern['frequency']})")
            report.append("**å½±éŸ¿ã‚’å—ã‘ãŸãƒ†ã‚¹ãƒˆ:**")
            for test_id in pattern["affected_tests"][:3]:
                report.append(f"- {test_id}")
            if len(pattern["affected_tests"]) > 3:
                report.append(f"- ...ä»–{len(pattern['affected_tests']) - 3}ä»¶")
            report.append("")
        
        # æ”¹å–„ææ¡ˆ
        report.append("## ğŸ¯ å…·ä½“çš„æ”¹å–„ææ¡ˆ")
        high_priority = [s for s in analysis["improvement_suggestions"] if s.priority == "high"]
        for i, suggestion in enumerate(high_priority, 1):
            report.append(f"### {i}. {suggestion.problem}")
            report.append(f"**ã‚«ãƒ†ã‚´ãƒª**: {suggestion.category}")
            report.append(f"**ä¿®æ­£å¯¾è±¡**: {suggestion.file_to_modify}")
            report.append(f"**ææ¡ˆå†…å®¹**: {suggestion.suggested_fix}")
            report.append(f"**è¨¼æ‹ **: {len(suggestion.evidence)}ä»¶ã®ãƒ†ã‚¹ãƒˆã§ç¢ºèª")
            report.append("")
        
        # è©³ç´°ãƒ†ã‚¹ãƒˆçµæœ
        report.append("## ğŸ“‹ è©³ç´°ãƒ†ã‚¹ãƒˆçµæœ")
        for result in test_results:
            status = "âœ…" if result.overall_success else "âŒ"
            report.append(f"### {status} {result.test_id}: {result.scenario_name}")
            report.append(f"**ä¿¡é ¼åº¦**: {result.confidence_score:.2f}")
            
            # åˆ¤å®šè©³ç´°
            report.append("**è©³ç´°åˆ¤å®š:**")
            for key, judgement in result.judgements.items():
                report.append(f"- {key}: {judgement}")
            
            if result.issues:
                report.append("**å•é¡Œç‚¹:**")
                for issue in result.issues:
                    report.append(f"- {issue}")
            
            report.append("")
        
        return "\n".join(report)

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def run_example_test():
    """å®Ÿè¡Œä¾‹"""
    validator = DetailedValidator()
    analyzer = AnalysisEngine()
    reporter = TestReportGenerator()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆçµæœ
    sample_results = [
        TestResult(
            test_id="APT-001",
            scenario_name="æ¨™æº–çš„ãªäºˆç´„æ¥å®¢",
            overall_success=True,
            confidence_score=0.95,
            extraction_scores={"name": 1.0, "company": 1.0, "visitor_type": 1.0},
            flow_scores={"state_transition": 1.0},
            quality_scores={"politeness": 0.9, "clarity": 0.8},
            judgements={
                "name_extraction": "âœ… æ­£ç¢º (ç”°ä¸­å¤ªéƒ)",
                "company_extraction": "âœ… æ­£ç¢º (ABCå•†äº‹)",
                "visitor_type_classification": "âœ… æ­£ç¢º (appointment)"
            },
            issues=[],
            evidence=TestEvidence(
                test_id="APT-001",
                input_message="ç”°ä¸­å¤ªéƒã§ã™ã€‚ABCå•†äº‹ã‹ã‚‰14æ™‚ã®å±±ç”°éƒ¨é•·ã¨ã®ä¼šè­°ã§æ¥ã¾ã—ãŸã€‚",
                expected={"name": "ç”°ä¸­å¤ªéƒ", "company": "ABCå•†äº‹", "visitor_type": "appointment"},
                actual={"name": "ç”°ä¸­å¤ªéƒ", "company": "ABCå•†äº‹", "visitor_type": "appointment"},
                passed=True,
                issues=[],
                timestamp=datetime.now().isoformat()
            ),
            suggestions=[]
        )
    ]
    
    # åˆ†æå®Ÿè¡Œ
    analysis = analyzer.analyze_test_results(sample_results)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = reporter.generate_detailed_report(analysis, sample_results)
    
    return report

if __name__ == "__main__":
    report = asyncio.run(run_example_test())
    print(report)