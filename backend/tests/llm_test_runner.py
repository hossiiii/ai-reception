"""
LLMãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ - å®Ÿéš›ã®APIã¨é€£æºã—ã¦ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯:
1. å®Ÿéš›ã®AIå—ä»˜ã‚·ã‚¹ãƒ†ãƒ APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
2. ãƒ†ã‚¹ãƒˆçµæœã‚’è©³ç´°ã«åˆ†æ
3. å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ
"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

import aiohttp
import yaml

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from llm_test_framework import (
    AnalysisEngine,
    DetailedValidator,
    ExtractionResult,
    ImprovementSuggestion,
    LLMTestResult,
    TestEvidence,
    TestReportGenerator,
)


class APITestClient:
    """AIå—ä»˜ã‚·ã‚¹ãƒ†ãƒ APIã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session: aiohttp.ClientSession | None = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def health_check(self) -> bool:
        """APIã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            async with self.session.get(f"{self.base_url}/api/health") as response:
                return response.status == 200
        except Exception as e:
            print(f"Health check failed: {e}")
            return False

    async def start_conversation(self) -> str | None:
        """æ–°ã—ã„ä¼šè©±ã‚’é–‹å§‹"""
        try:
            async with self.session.post(f"{self.base_url}/api/conversations/") as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get("session_id")
                else:
                    print(f"Failed to start conversation: {response.status}")
                    return None
        except Exception as e:
            print(f"Error starting conversation: {e}")
            return None

    async def send_message(self, session_id: str, message: str) -> dict[str, Any] | None:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        try:
            async with self.session.post(
                f"{self.base_url}/api/conversations/{session_id}/messages",
                json={"message": message}
            ) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    print(f"Failed to send message: {response.status}")
                    return None
        except Exception as e:
            print(f"Error sending message: {e}")
            return None

    async def get_conversation_history(self, session_id: str) -> dict[str, Any] | None:
        """ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
        try:
            async with self.session.get(f"{self.base_url}/api/conversations/{session_id}") as response:
                if response.status == 200:
                    return await response.json()
                else:
                    print(f"Failed to get conversation history: {response.status}")
                    return None
        except Exception as e:
            print(f"Error getting conversation history: {e}")
            return None

class LLMTestRunner:
    """LLMãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã¨åˆ†æã‚’è¡Œã†ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self, scenarios_file: str = "test_scenarios.yaml"):
        # Resolve path relative to this file's directory
        if not Path(scenarios_file).is_absolute():
            scenarios_file = str(Path(__file__).parent / scenarios_file)
        self.scenarios_file = scenarios_file
        self.validator = DetailedValidator()
        self.analyzer = AnalysisEngine()
        self.reporter = TestReportGenerator()
        self.test_scenarios = self._load_scenarios()

    def _load_scenarios(self) -> dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.scenarios_file, encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"Scenarios file not found: {self.scenarios_file}")
            return {}
        except Exception as e:
            print(f"Error loading scenarios: {e}")
            return {}

    async def run_single_scenario(self, scenario_id: str, api_client: APITestClient) -> LLMTestResult | None:
        """å˜ä¸€ã‚·ãƒŠãƒªã‚ªã®å®Ÿè¡Œ"""
        scenario = self._find_scenario(scenario_id)
        if not scenario:
            print(f"Scenario not found: {scenario_id}")
            return None

        print(f"Running scenario: {scenario_id} - {scenario['name']}")

        # ä¼šè©±é–‹å§‹
        session_id = await api_client.start_conversation()
        if not session_id:
            return self._create_error_result(scenario_id, "Failed to start conversation")

        conversation_results = []
        overall_success = True
        all_issues = []
        all_suggestions = []

        # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        for step_data in scenario.get("conversation", []):
            step_result = await self._execute_step(step_data, session_id, api_client)
            conversation_results.append(step_result)

            if not step_result.get("success", False):
                overall_success = False
                all_issues.extend(step_result.get("issues", []))
                all_suggestions.extend(step_result.get("suggestions", []))

        # æœ€çµ‚çš„ãªä¼šè©±å±¥æ­´ã‚’å–å¾—
        final_history = await api_client.get_conversation_history(session_id)

        # ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æ
        return self._analyze_scenario_result(
            scenario_id, scenario, conversation_results,
            final_history, overall_success, all_issues, all_suggestions
        )

    async def _execute_step(self, step_data: dict[str, Any], session_id: str, api_client: APITestClient) -> dict[str, Any]:
        """å˜ä¸€ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œ"""
        user_input = step_data.get("user_input")
        expected = step_data.get("expected", {})

        if not user_input:
            return {"success": False, "error": "No user input specified"}

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
        response = await api_client.send_message(session_id, user_input)
        if not response:
            return {"success": False, "error": "Failed to send message"}

        # å¿œç­”ã®æ¤œè¨¼
        extraction_result = self._extract_info_from_response(response)
        extraction_scores, extraction_judgements, extraction_issues = self.validator.validate_extraction(
            extraction_result, expected
        )

        # å¿œç­”å“è³ªã®æ¤œè¨¼
        response_text = response.get("message", "")
        quality_scores, quality_judgements, quality_issues = self.validator.validate_response_quality(response_text)

        # å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        keyword_issues = self._check_required_keywords(response_text, expected.get("must_include_keywords", []))

        # ã‚¹ãƒ†ãƒƒãƒ—æˆåŠŸåˆ¤å®š (ã‚ˆã‚Šç¾å®Ÿçš„ãªåŸºæº–ã«èª¿æ•´)
        extraction_passed = len(extraction_scores) == 0 or all(score >= 0.7 for score in extraction_scores.values())
        quality_passed = len(quality_scores) == 0 or all(score >= 0.7 for score in quality_scores.values())

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ã¯å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã®ã¿
        required_keywords = expected.get("must_include_keywords", [])
        keywords_passed = len(required_keywords) == 0 or len(keyword_issues) <= len(required_keywords) // 2  # åŠåˆ†ä»¥ä¸Šã‚ã‚Œã°OK

        step_success = extraction_passed and quality_passed and keywords_passed

        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        suggestions = self._generate_step_suggestions(
            extraction_issues + quality_issues + keyword_issues,
            step_data.get("step", 0)
        )

        return {
            "success": step_success,
            "step": step_data.get("step", 0),
            "user_input": user_input,
            "response": response,
            "extraction_scores": extraction_scores,
            "quality_scores": quality_scores,
            "judgements": {**extraction_judgements, **quality_judgements},
            "issues": extraction_issues + quality_issues + keyword_issues,
            "suggestions": suggestions
        }

    def _extract_info_from_response(self, response: dict[str, Any]) -> ExtractionResult:
        """APIå¿œç­”ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        visitor_info = response.get("visitor_info", {})

        # purposeã‹ã‚‰visitor_typeã‚’æ¨æ¸¬
        purpose = visitor_info.get("purpose", "")
        visitor_type = self._infer_visitor_type_from_purpose(purpose)

        # confidenceã®å¤‰æ›
        confidence_raw = visitor_info.get("confidence", "low")
        confidence_score = self._convert_confidence_to_score(confidence_raw)

        return ExtractionResult(
            name=visitor_info.get("name"),
            company=visitor_info.get("company"),
            visitor_type=visitor_type,
            purpose=purpose,
            confidence=confidence_score
        )

    def _infer_visitor_type_from_purpose(self, purpose: str) -> str:
        """purposeã‹ã‚‰visitor_typeã‚’æ¨æ¸¬"""
        purpose = purpose.lower()

        # å–¶æ¥­é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯
        if any(keyword in purpose for keyword in ["å–¶æ¥­", "æ¡ˆå†…", "ç´¹ä»‹", "ææ¡ˆ", "å•†å“", "ã‚µãƒ¼ãƒ“ã‚¹", "æ–°å•†å“", "è²©å£²"]):
            return "sales"
        elif any(keyword in purpose for keyword in ["é…é”", "ãŠå±Šã‘", "è·ç‰©", "å®…é…"]):
            return "delivery"
        elif any(keyword in purpose for keyword in ["ä¼šè­°", "ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "æ‰“ã¡åˆã‚ã›", "é¢è«‡", "äºˆç´„"]):
            return "appointment"
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯äºˆç´„ã¨ã—ã¦åˆ¤å®š
            return "appointment"

    def _convert_confidence_to_score(self, confidence_raw: str) -> float:
        """confidenceæ–‡å­—åˆ—ã‚’ã‚¹ã‚³ã‚¢ã«å¤‰æ›"""
        confidence_map = {
            "high": 0.9,
            "medium": 0.7,
            "low": 0.5
        }
        return confidence_map.get(confidence_raw.lower(), 0.5)

    def _check_required_keywords(self, text: str, required_keywords: list[str]) -> list[str]:
        """å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯ï¼ˆæŸ”è»Ÿãªä¸€è‡´ã‚’è¨±å¯ï¼‰"""
        issues = []

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼šæœŸå¾…ã•ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â†’ è¨±å¯ã•ã‚Œã‚‹ä»£æ›¿è¡¨ç¾
        keyword_alternatives = {
            "ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ": ["ç¢ºèªã„ãŸã—ã¾ã—ãŸ", "ç¢ºèªã—ãŸ", "è¦‹å½“ãŸã‚Šã¾ã›ã‚“", "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"],
            "ãŠå¾…ã¡": ["å°‘ã€…ãŠå¾…ã¡", "ã—ã°ã‚‰ããŠå¾…ã¡", "ãŠå‘¼ã³", "ã”æ¡ˆå†…"],
            "æ‹…å½“è€…": ["æ‹…å½“", "è²¬ä»»è€…", "ã‚¹ã‚¿ãƒƒãƒ•"],
            "å—ä»˜": ["ãƒ•ãƒ­ãƒ³ãƒˆ", "çª“å£", "ã“ã¡ã‚‰"],
            "å–¶æ¥­": ["è²©å£²", "å•†è«‡", "ãƒ“ã‚¸ãƒã‚¹"],
            "ã”ç”¨ä»¶": ["ã”è¦ä»¶", "ç›®çš„", "ä»¶"],
            "è·ç‰©": ["ãŠè·ç‰©", "é…é€ç‰©", "å®…é…ä¾¿"],
            "ã‚µã‚¤ãƒ³": ["ç½²å", "å—ã‘å–ã‚Š", "ç¢ºèª"],
            "ãŠåå‰": ["åå‰", "æ°å"],
            "ä¼šç¤¾å": ["ä¼šç¤¾", "ä¼æ¥­å", "æ³•äººå"],
            "ã”ç”¨ä»¶": ["ç”¨ä»¶", "ç›®çš„", "è¦ä»¶"],
            "ãŠèã‹ã›ãã ã•ã„": ["æ•™ãˆã¦", "ãŠæ•™ãˆ", "èã‹ã›ã¦"],
            "ã‚‚ã†ä¸€åº¦": ["å†åº¦", "ã‚‚ã†ä¸€å›"],
            "ç”³ã—è¨³": ["ã™ã¿ã¾ã›ã‚“", "ã”ã‚ã‚“", "å¤±ç¤¼"],
            "ãŠæ‰‹ä¼ã„": ["ã‚µãƒãƒ¼ãƒˆ", "æ”¯æ´", "æ‰‹åŠ©ã‘"]
        }

        for keyword in required_keywords:
            # ç›´æ¥ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
            if keyword in text:
                continue

            # ä»£æ›¿è¡¨ç¾ã‚’ãƒã‚§ãƒƒã‚¯
            alternatives = keyword_alternatives.get(keyword, [])
            if any(alt in text for alt in alternatives):
                continue

            # ã©ã‚Œã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
            issues.append(f"å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ãŒå¿œç­”ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        return issues

    def _generate_step_suggestions(self, issues: list[str], step: int) -> list[ImprovementSuggestion]:
        """ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ã®æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        suggestions = []

        for issue in issues:
            if "åå‰æŠ½å‡º" in issue:
                suggestions.append(ImprovementSuggestion(
                    category="prompt",
                    problem="åå‰æŠ½å‡ºã®ç²¾åº¦å‘ä¸ŠãŒå¿…è¦",
                    evidence=[f"Step {step}: {issue}"],
                    suggested_fix="ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åå‰æŠ½å‡ºã®ä¾‹æ–‡ã‚’è¿½åŠ ",
                    file_to_modify="app/agents/nodes.py",
                    priority="high"
                ))
            elif "visitor_type" in issue:
                suggestions.append(ImprovementSuggestion(
                    category="logic",
                    problem="è¨ªå•è€…ã‚¿ã‚¤ãƒ—åˆ†é¡ã®æ”¹å–„ãŒå¿…è¦",
                    evidence=[f"Step {step}: {issue}"],
                    suggested_fix="è¨ªå•è€…ã‚¿ã‚¤ãƒ—åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„",
                    file_to_modify="app/agents/nodes.py",
                    priority="high"
                ))
            elif "é•·ã™ãã‚‹" in issue:
                suggestions.append(ImprovementSuggestion(
                    category="config",
                    problem="å¿œç­”ã®é•·ã•åˆ¶å¾¡ãŒå¿…è¦",
                    evidence=[f"Step {step}: {issue}"],
                    suggested_fix="max_tokensè¨­å®šã‚’150ã«å¤‰æ›´",
                    file_to_modify="app/services/text_service.py",
                    priority="medium"
                ))
            elif "æ•¬èª" in issue:
                suggestions.append(ImprovementSuggestion(
                    category="prompt",
                    problem="æ•¬èªä½¿ç”¨ã®å¾¹åº•ãŒå¿…è¦",
                    evidence=[f"Step {step}: {issue}"],
                    suggested_fix="ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä¸å¯§èªä½¿ç”¨ã‚’å¼·èª¿",
                    file_to_modify="app/agents/nodes.py",
                    priority="medium"
                ))

        return suggestions

    def _analyze_scenario_result(
        self, scenario_id: str, scenario: dict[str, Any],
        conversation_results: list[dict[str, Any]], final_history: dict[str, Any] | None,
        overall_success: bool, all_issues: list[str], all_suggestions: list[ImprovementSuggestion]
    ) -> LLMTestResult:
        """ã‚·ãƒŠãƒªã‚ªçµæœã®åˆ†æ"""

        # å…¨ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆ
        all_extraction_scores = {}
        all_quality_scores = {}
        all_judgements = {}

        for result in conversation_results:
            all_extraction_scores.update(result.get("extraction_scores", {}))
            all_quality_scores.update(result.get("quality_scores", {}))
            all_judgements.update(result.get("judgements", {}))

        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        confidence_score = self._calculate_confidence_score(all_extraction_scores, all_quality_scores)

        # è¨¼è·¡ä½œæˆ
        evidence = TestEvidence(
            test_id=scenario_id,
            input_message=conversation_results[0].get("user_input", "") if conversation_results else "",
            expected=scenario.get("conversation", [{}])[0].get("expected", {}),
            actual=self._extract_actual_from_results(conversation_results),
            passed=overall_success,
            issues=all_issues,
            timestamp=datetime.now().isoformat()
        )

        return LLMTestResult(
            test_id=scenario_id,
            scenario_name=scenario.get("name", ""),
            overall_success=overall_success,
            confidence_score=confidence_score,
            extraction_scores=all_extraction_scores,
            flow_scores={},  # TODO: ãƒ•ãƒ­ãƒ¼ã‚¹ã‚³ã‚¢ã®å®Ÿè£…
            quality_scores=all_quality_scores,
            judgements=all_judgements,
            issues=all_issues,
            evidence=evidence,
            suggestions=all_suggestions
        )

    def _calculate_confidence_score(self, extraction_scores: dict[str, float], quality_scores: dict[str, float]) -> float:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        all_scores = list(extraction_scores.values()) + list(quality_scores.values())
        return sum(all_scores) / len(all_scores) if all_scores else 0.0

    def _extract_actual_from_results(self, conversation_results: list[dict[str, Any]]) -> dict[str, Any]:
        """ä¼šè©±çµæœã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’æŠ½å‡º"""
        if not conversation_results:
            return {}

        first_result = conversation_results[0]
        response = first_result.get("response", {})
        visitor_info = response.get("visitor_info", {})

        return {
            "name": visitor_info.get("name"),
            "company": visitor_info.get("company"),
            "visitor_type": visitor_info.get("visitor_type"),
            "current_step": response.get("step")
        }

    def _find_scenario(self, scenario_id: str) -> dict[str, Any] | None:
        """ã‚·ãƒŠãƒªã‚ªIDã‹ã‚‰è©²å½“ã‚·ãƒŠãƒªã‚ªã‚’æ¤œç´¢"""
        for _category, scenarios in self.test_scenarios.get("test_scenarios", {}).items():
            for scenario in scenarios:
                if scenario["id"] == scenario_id:
                    return scenario
        return None

    def _create_error_result(self, scenario_id: str, error_message: str) -> LLMTestResult:
        """ã‚¨ãƒ©ãƒ¼ç”¨ã®ãƒ†ã‚¹ãƒˆçµæœã‚’ä½œæˆ"""
        return LLMTestResult(
            test_id=scenario_id,
            scenario_name="Error",
            overall_success=False,
            confidence_score=0.0,
            extraction_scores={},
            flow_scores={},
            quality_scores={},
            judgements={"error": f"âŒ {error_message}"},
            issues=[error_message],
            evidence=TestEvidence(
                test_id=scenario_id,
                input_message="",
                expected={},
                actual={},
                passed=False,
                issues=[error_message],
                timestamp=datetime.now().isoformat()
            ),
            suggestions=[]
        )

    async def run_test_suite(self, scenario_ids: list[str] | None = None) -> list["LLMTestResult"]:
        """ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ"""
        if scenario_ids is None:
            # å…¨ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ
            scenario_ids = []
            for _category, scenarios in self.test_scenarios.get("test_scenarios", {}).items():
                scenario_ids.extend([s["id"] for s in scenarios])

        results = []

        async with APITestClient() as api_client:
            # APIå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if not await api_client.health_check():
                print("API health check failed. Cannot run tests.")
                return results

            print(f"Running {len(scenario_ids)} test scenarios...")

            # å„ã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ
            for scenario_id in scenario_ids:
                try:
                    result = await self.run_single_scenario(scenario_id, api_client)
                    if result:
                        results.append(result)

                    # å°‘ã—å¾…æ©Ÿï¼ˆAPIè² è·è»½æ¸›ï¼‰
                    await asyncio.sleep(0.5)

                except Exception as e:
                    print(f"Error running scenario {scenario_id}: {e}")
                    results.append(self._create_error_result(scenario_id, str(e)))

        return results

    async def run_and_report(self, scenario_ids: list[str] | None = None, output_file: str | None = None):
        """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("ğŸš€ LLMãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
        print("=" * 50)

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        results = await self.run_test_suite(scenario_ids)

        if not results:
            print("âŒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return

        # çµæœåˆ†æ
        analysis = self.analyzer.analyze_test_results(results)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.reporter.generate_detailed_report(analysis, results)

        # çµæœå‡ºåŠ›
        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå®Œäº†")
        print(f"å®Ÿè¡Œãƒ†ã‚¹ãƒˆæ•°: {len(results)}")
        print(f"æˆåŠŸç‡: {analysis['overall_metrics']['success_rate']:.1%}")

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_file}")
        else:
            print("\n" + "=" * 50)
            print(report)

# å®Ÿè¡Œä¾‹
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = LLMTestRunner()

    # ç‰¹å®šã®ã‚·ãƒŠãƒªã‚ªã‚’ãƒ†ã‚¹ãƒˆ
    # await runner.run_and_report(["APT-001", "SALES-001", "ERR-001"])

    # å…¨ã‚·ãƒŠãƒªã‚ªã‚’ãƒ†ã‚¹ãƒˆ
    await runner.run_and_report(output_file="llm_test_report.md")

if __name__ == "__main__":
    asyncio.run(main())
