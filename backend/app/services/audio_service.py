import asyncio
import io
from typing import Protocol

from openai import AsyncOpenAI

from ..config import settings
from .connection_pool import get_connection_pool
from .simple_audio_cache import get_audio_cache


class AudioProcessor(Protocol):
    """Audio processing protocol for Step2 voice functionality"""

    async def process_audio_input(self, audio_data: bytes) -> str:
        """Convert audio to text"""
        ...

    async def generate_audio_output(self, text: str) -> bytes:
        """Convert text to audio"""
        ...


class AudioService:
    """Step2: Audio processing service that extends Step1's MessageProcessor architecture"""

    def __init__(self):
        # Initialize audio cache
        self.audio_cache = get_audio_cache()
        
        # Use same settings pattern as Step1's TextService
        if settings.openai_api_key and settings.openai_api_key.startswith('sk-'):
            # Use individual OpenAI client with optimized timeout
            self.openai_client = AsyncOpenAI(
                api_key=settings.openai_api_key,
                timeout=5.0  # Reduced timeout for better performance
            )
            self.use_mock = False
            print(f"âœ… AudioService initialized with OpenAI API and cache")
        else:
            self.openai_client = None
            self.use_mock = True
            print("âš ï¸ AudioService initialized with mock mode (no valid OpenAI API key)")

    async def process_input(self, input_data: str) -> str:
        """Step1 compatibility: Process text input as-is"""
        return input_data.strip()

    async def process_audio_input(self, audio_data: bytes) -> str:
        """Step2: Convert audio data to text using OpenAI Whisper API"""
        if self.use_mock:
            # Add realistic delay to simulate API call
            await asyncio.sleep(0.8)
            return "ã“ã‚“ã«ã¡ã¯ã€ãƒ†ã‚¹ãƒˆéŸ³å£°ã§ã™ã€‚å±±ç”°å¤ªéƒã€æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆã§ã™ã€‚"

        try:
            # Create audio file for OpenAI Whisper API
            audio_file = io.BytesIO(audio_data)
            audio_file.name = "audio.webm"  # WebM format from frontend

            print(f"ğŸ¤ Processing audio input ({len(audio_data)} bytes)...")

            transcript = await self.openai_client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ja",  # Japanese language specification
                temperature=0.0  # Deterministic transcription
            )

            transcribed_text = transcript.text
            print(f"âœ… Audio transcribed: {transcribed_text[:50]}...")

            return transcribed_text

        except Exception as e:
            print(f"âŒ Audio transcription error: {e}")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚éŸ³å£°ãŒèãå–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©±ã—ãã ã•ã„ã€‚"

    async def generate_output(self, text: str, context: str = "") -> str:
        """Step1 compatibility: Generate text response using same logic as TextService"""

        # Use mock response in development mode
        if self.use_mock:
            return await self._generate_mock_response(text, context)

        try:
            system_prompt = """ã‚ãªãŸã¯ä¸å¯§ã§åŠ¹ç‡çš„ãªå—ä»˜AIã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å¿œç­”ã—ã¦ãã ã•ã„:

1. å¸¸ã«æ•¬èªã‚’ä½¿ç”¨ã—ã€ä¸å¯§ã«å¯¾å¿œã™ã‚‹
2. æ¥å®¢è€…ã®åå‰ã¨ä¼šç¤¾åã‚’æ­£ç¢ºã«ç¢ºèªã™ã‚‹
3. æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€å…·ä½“çš„ã«ä½•ãŒå¿…è¦ã‹ã‚’ä¼ãˆã‚‹
4. æ¥å®¢è€…ã®ã‚¿ã‚¤ãƒ—ï¼ˆäºˆç´„ã€å–¶æ¥­ã€é…é”ï¼‰ã‚’åˆ¤æ–­ã—ã¦é©åˆ‡ã«æ¡ˆå†…ã™ã‚‹
5. å¿œç­”ã¯ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãã€æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨ã‚’ç¤ºã™
6. æ—¥æœ¬èªã§å¿œç­”ã™ã‚‹
7. å—ä»˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦é©åˆ‡ãªè³ªå•ã‚„æ¡ˆå†…ã‚’è¡Œã†
8. éŸ³å£°ã§ã®å¯¾è©±ã‚’å‰æã¨ã—ã¦ã€è‡ªç„¶ã§èãå–ã‚Šã‚„ã™ã„å¿œç­”ã‚’å¿ƒãŒã‘ã‚‹"""

            if context:
                system_prompt += f"\n\nè¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}"

            # Create conversation messages
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text}
            ]

            print(f"ğŸ¤– Generating response for: {text[:50]}...")

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # Same model as TextService
                messages=messages,
                max_tokens=200,
                temperature=0.7,
                timeout=5  # Reduced timeout from 15s to 5s for better performance
            )

            ai_response = response.choices[0].message.content or ""
            print(f"âœ… Generated response: {ai_response[:50]}...")

            return ai_response

        except Exception as e:
            print(f"âŒ Text generation error: {e}")
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

    async def generate_audio_output(self, text: str, voice: str = "alloy") -> bytes:
        """Step2: Convert text to audio using OpenAI TTS API with caching"""
        
        # Check cache first
        cached_audio = self.audio_cache.get(text, voice)
        if cached_audio:
            return cached_audio
        
        if self.use_mock:
            # Add realistic delay to simulate API call
            await asyncio.sleep(0.6)
            # Return mock audio data (empty bytes with size indicator)
            mock_audio = b"mock_audio_data_" + text.encode('utf-8')[:20] + b"_end"
            self.audio_cache.set(text, mock_audio, voice)
            return mock_audio

        try:
            print(f"ğŸ”Š Generating audio for: {text[:50]}...")

            response = await self.openai_client.audio.speech.create(
                model="gpt-4o-mini-tts",  # Corrected TTS model name
                voice=voice,   # Clear, neutral voice suitable for Japanese
                input=text,
                response_format="wav"  # WAV format for broad compatibility
            )

            audio_data = response.content
            print(f"âœ… Audio generated ({len(audio_data)} bytes)")
            
            # Store in cache
            self.audio_cache.set(text, audio_data, voice)

            return audio_data

        except Exception as e:
            print(f"âŒ Audio generation error: {e}")
            return b""  # Return empty bytes on error

    async def _generate_mock_response(self, text: str, context: str = "") -> str:
        """Generate mock AI response for development mode (same as TextService)"""
        # Add small delay to simulate API call
        await asyncio.sleep(0.5)

        # Simple mock responses based on context
        if "åå‰" in text and "ä¼šç¤¾" in text:
            return """ç¢ºèªã„ãŸã—ã¾ã™ã€‚

ãŠåå‰ã¨ã”æ‰€å±ã‚’ã€ã‚‚ã†ä¸€åº¦ãŠèã‹ã›ãã ã•ã„ã€‚"""

        elif "ã¯ã„" in text or "yes" in text.lower():
            return """æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚

ã”æ¥è¨ªã®ç›®çš„ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚ã”äºˆç´„ã€å–¶æ¥­ã€é…é”ãªã©ã”ç”¨ä»¶ã‚’ãŠæ•™ãˆãã ã•ã„ã€‚"""

        else:
            return """ã„ã‚‰ã£ã—ã‚ƒã„ã¾ã›ã€‚éŸ³å£°å—ä»˜ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

ä¼šç¤¾åã€ãŠåå‰ã€ã”ç”¨ä»¶ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚"""


# Maintain Step1 compatibility by providing the same interface
MessageProcessorImpl = AudioService
